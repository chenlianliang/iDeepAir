{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, GRU, Multiply, Reshape\n",
    "from keras.layers import RepeatVector, Dense, Activation, Lambda, Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "import keras\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "# from tcn import TCN\n",
    "%matplotlib inline\n",
    "from keras.layers import merge, Input, Dense, TimeDistributed, Lambda                                   \n",
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, GlobalMaxPooling2D,MaxPooling2D, Flatten, Dense, BatchNormalization, Activation, Input, GlobalAveragePooling2D,AveragePooling2D, Add\n",
    "from keras.optimizers import Adam, rmsprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.constraints import maxnorm\n",
    "MAX_SENT_LENGTH = 55\n",
    "MAX_SENTS = 24\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler  \n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from pandas import concat\n",
    "\n",
    "std = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#matplotlib inline\n",
    "#参数初始化\n",
    "discfile = '../data/data_wea_time_unify.csv'\n",
    "\n",
    "data = pd.read_csv(discfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import os\n",
    "\n",
    "ground = h5py.File('../data/inte_g_12_linear.h5', 'r')\n",
    "g = ground['X']\n",
    "\n",
    "ele_zs = h5py.File('../data/inte_e_zs_12_new_linear.h5', 'r')\n",
    "ezs = ele_zs['X']\n",
    "\n",
    "ele_sd = h5py.File('../data/inte_e_sd_12_new_linear.h5', 'r')\n",
    "esd = ele_sd['X']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_m=[g,ezs,esd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_m=np.array(g_m).reshape(6064,16,16,3)\n",
    "g=np.array(g).reshape(6064,16,16,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from attention_keras import *\n",
    "from keras.layers import LeakyReLU\n",
    "class AttentionLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(** kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        # W.shape = (time_steps, time_steps)\n",
    "        self.W = self.add_weight(name='att_weight', \n",
    "                                 shape=(input_shape[1], input_shape[1]),\n",
    "                                 initializer='uniform',\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        x = K.permute_dimensions(inputs, (0, 2, 1))\n",
    "        # x.shape = (batch_size, seq_len, time_steps)\n",
    "        # general\n",
    "        a = K.softmax(K.tanh(K.dot(x, self.W)))\n",
    "        a = K.permute_dimensions(a, (0, 2, 1))\n",
    "        outputs = a * inputs\n",
    "        outputs = K.sum(outputs, axis=1)\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[2]\n",
    "\n",
    "def create_model(learn_rate=0.001, neurons=24, traffic_n=3):    \n",
    "#     交通指数数据模块\n",
    "    inp=Input(shape=(16, 16, 1))\n",
    "    out=Conv2D(filters=1, kernel_size=3, strides=1)(out)\n",
    "    out=MaxPooling2D(pool_size=(3, 3), strides=2)(out)\n",
    "    out=Flatten()(out)\n",
    "\n",
    "    out=Dense(3)(out)\n",
    "    out=Dense(1)(out)\n",
    "    tra=Model(inputs=inp, outputs=out)\n",
    "    \n",
    "#     特征组件\n",
    "    sentence_input = Input(shape=(54+traffic_n,))\n",
    "    # s_input=keras.layers.concatenate([sentence_input])\n",
    "    embed = Reshape((54+traffic_n,1))(sentence_input)\n",
    "    l_lstm=MAttention(8,16)([embed,embed,embed])\n",
    "    l_dense = TimeDistributed(Dense(32))(l_lstm)\n",
    "\n",
    "    l_att = AttentionLayer()(l_dense)\n",
    "\n",
    "    sentEncoder = Model(sentence_input, l_att)\n",
    "\n",
    "    \n",
    "    review_input = Input(shape=(MAX_SENTS,54))\n",
    "    r_input = Input(shape=(MAX_SENTS,16, 16, 3))\n",
    "    r_encoder = TimeDistributed(tra)(r_input)\n",
    "    s_input=keras.layers.concatenate([r_encoder,review_input])\n",
    "    \n",
    "    review_encoder = TimeDistributed(sentEncoder)(s_input)\n",
    "    w_input=Input(shape=(MAX_SENTS,3))\n",
    "    w_encoder = TimeDistributed(wm)(w_input)\n",
    "\n",
    "    wea_input=Input(shape=(MAX_SENTS,5))\n",
    "    wea_encoder = TimeDistributed(weam)(wea_input)\n",
    "    l_lstm_sent=TCN(padding='same',return_sequences=True)(review_encoder)\n",
    "    we_time_encoder=keras.layers.concatenate([w_encoder,wea_input,l_lstm_sent])\n",
    "\n",
    "    l_dense_sent = TimeDistributed(Dense(64))(we_time_encoder)\n",
    "    l_att_sent = AttentionLayer()(l_dense_sent)\n",
    "    preds = Dense(futrue_time)(l_att_sent)\n",
    "    model_1 = Model([w_input,wea_input,r_input,review_input], preds)\n",
    "    model_1.compile(optimizer=Adam(lr=learn_rate, beta_1=0.99, beta_2=0.999, decay=0.0006),\n",
    "                        loss='mae')\n",
    "    return model_1\n",
    "\n",
    "model=create_model()\n",
    "history=model.fit([w_m,wea_m,ge_m,sd_m],td, epochs=20, batch_size=68,verbose=2,validation_split=0.2,shuffle=True)\n",
    "# 绘制训练 & 验证的损失值\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
